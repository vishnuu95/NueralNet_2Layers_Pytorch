{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_vishnuu.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKOkstRh+8mxrISiMDBCYz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BXZUOrZ9CMmJ","colab_type":"text"},"source":["# **Implementing a 2 Layer Fully connected Nueral Net using Standard Libraries(pytorch) for classification**."]},{"cell_type":"markdown","metadata":{"id":"rsUwR-CN3zfg","colab_type":"text"},"source":["\n","\n","## 1.  KEY TAKE AWAYS\n","\n","\n","* There was no real linear trend found.\n","*   It is good to overfit our model first and then fit it correctly.\n","*   Tuning the hyperparameters is far from intuitive. Accuracies and loss trends are linear only in small neighborhood.\n","*   Accuracy and Loss for training / testing need not go hand in hand. Once, it was noticed that training higher, but training accuracy was better.\n","*   Using Momentum helped converge rapidly.\n","*   Reducing batch size also helped converge quickly.\n","*   Early stopping in cases when Loss was stagnant didn't seem to help much. \n","*   Having some momentum did help converge faster but it also overfit the model.\n","*   The same accuracy can be achieved with a diverse set of hyperparameters.\n","* L2 regularization was key to avoid overfitting. \n","\n","\n","\n","## 2.  THINGS TO EXPLORE / UNANSWERED QUESTIONS\n","\n","* Not sure why augmentation of data didn't help much(horizontal flip).\n","* Would  (lr + zero momentum for high epochs) be better than (lr + high momentum for lower epochs) to avoid overfitting. \n","* Not sure why shuffling of data reduced testing accuracy (it did reduce testing loss though).\n","\n","## 3. LIST OF METHODS EXPLORED\n","1. **Initializations** : All initializations were random initializations with certain modifications.\n","* np.rand(fanin, fanout)/100 # tag: divide_by_100.\n","* np.rand(fanin, fanout)/sqrt(fanin*fanout) # tag: divide_by_sqrt.\n","* np.rand(fanin, fanout)/(fanin*fanout) # tag: divide_by_prod.\n","\n","---\n","2. **Altering Learning rate, Momentum while using SGD** \\\n","* Learning rate and momentum was altered simultaneously while using SGD-M.\n","---\n","3. **Batch size and Epochs** \\\n","* Batch size was altered in factors of the data set size. \n","* Epochs were tried from range of 1000 - 4000. \n","---\n","4. **Activation Functions** \\\n","* ReLU.\n","* Leaky ReLU(0.01).\n","* Softplus(beta = 1, threshold = 5).\n","---\n","5. **Optimizers**\n","* SGDM.\n","* ADAM.\n","---\n","6. **Data Manipulation**\n","* Shuffling of data between epochs.\n","* Data augmentation using Horizontal Flipping of images\n","* Normalizing the feature vector. \n","---\n","\n","## 4. RESULTS AND DISCUSSION\n","\n","* For most of the trials, only a single hyperparameter was altered to see the effect clearly. \n","* Below is a summary of results and analysis of impact of each parameter/ hyperparameter on the losses and accuracies found. The entire list of results can be found [here](https://drive.google.com/drive/folders/1CUuDQfQU6CgQ2jocpdKn8xIWwUGu8I8C?usp=sharing)\n","---\n","###  CAT vs NON-CAT CLASSIFIER. \n","\n","* BEST OVERALL RESULT\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augment** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.3561758264 | 0.4837650061 | 0.8947368421 | 0.82 | Shuffle=False | Normalize=False | Augment=False |\n","\n","---\n","\n","1.   Effect of Initialization: \n","\n","* It was seen that the convergence didn't depend much on the initialisation. \n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_sqrt\\_prod | 209 | 3000 | 0.0075 | 0.95 | 0.18 | 0.2347040419 | 0.6292902231 | 0.980861244 | 0.76 |\n","| divide\\_by\\_100 | 209 | 3000 | 0.0075 | 0.95 | 0.18 | 0.2347040419 | 0.6292902231 | 0.980861244 | 0.76 |\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_100 | 19 | 3000 | 0.00025 | 0.8 | 0.15 | 0.223818798 | 0.6097765565 | 0.961722488 | 0.72 |\n","| divide\\_by\\_prod | 19 | 3000 | 0.00025 | 0.8 | 0.15 | 0.223818798 | 0.6097765565 | 0.961722488 | 0.72 |\n","\n","\n","2. Altering Learning rate and Momentum in SGD:\n","* For the range of epochs in interest, in most of the cases, reducing learning rate also reduced the loss(exception when the learning rate is set really low lr = 0.0001 - 0.0005).\n","* Increasing the learning rate helped converge quickly to a particular loss but need not necessarily be desirable. \n","* Some trends are shown below. \n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 19 | 3000 | 0.0009 | 0.65 | 0.13 | 0.2087284178 | 0.5798214674 | 0.9330143541 | 0.7 |\n","| divide\\_by\\_prod | 19 | 3000 | 0.0025 | 0.65 | 0.13 | 0.2157168673 | 0.5547247529 | 0.8755980861 | 0.78 |\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.7 | 0.13 | 0.3584286625 | 0.5311695933 | 0.9043062201 | 0.8 |\n","| divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.75 | 0.13 | 0.3686642213 | 0.6165066361 | 0.9138755981 | 0.62 |\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_100 | 19 | 3000 | 0.0002 | 0.8 | 0.18 | 0.2603222497 | 0.5928133726 | 0.95215311 | 0.72 |\n","| divide\\_by\\_100 | 19 | 3000 | 0.0002 | 0.9 | 0.18 | 0.2597359717 | 0.6020284295 | 0.956937799 | 0.72 |\n","| divide\\_by\\_100 | 19 | 3000 | 0.0002 | 1 | 0.15 | 0.2733703445 | 0.7973586917 | 0.8899521531 | 0.64 |\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 19 | 3000 | 0.0004 | 0.8 | 0.13 | 0.203201829 | 0.6146169305 | 0.966507177 | 0.72 |\n","| divide\\_by\\_prod | 19 | 3000 | 0.0004 | 0.85 | 0.13 | 0.2019675523 | 0.6121790409 | 0.966507177 | 0.72 |\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.15 | 0.3946999637 | 0.5608567595 | 0.9330143541 | 0.76 |\n","| divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.55 | 0.13 | 0.2069181827 | 0.6355372071 | 0.9473684211 | 0.74 |\n","\n","\n","\n","| Optimizer | Activation1 | weight\\_init | batch\\_size | epochs | learning\\_rate | momentum | weight\\_decay | train\\_loss | test\\_loss | train\\_acc | test\\_acc | Shuffle | Normalize |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.5 | 0.13 | 0.2074568719 | 0.6148176789 | 0.9473684211 | 0.7 | Shuffle=False | Normalize=False |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.3561758264 | 0.4837650061 | 0.8947368421 | 0.82 | Shuffle=False | Normalize=False |\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | softplus | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.2 | 0.2880446532 | 0.6287472844 | 0.971291866 | 0.74 | Shuffle=True | Normalize=False | Augment=False |\n","| sgdm | softplus | divide\\_by\\_prod | 19 | 2000 | 0.002 | 0.3 | 0.2 | 0.2732661245 | 0.6515020132 | 0.961722488 | 0.72 | Shuffle=True | Normalize=False | Augment=False |\n","\n","\n","\n","\n","3. Batch size and Epochs.\n","* It was generally seen that batch size helped converge faster and epochs sometimes reduced training loss. But most of the times, increase in epochs meant overfitting.  \n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_100 | 209 | 3000 | 0.007 | 0.9 | 0.15 | 0.2120349427 | 0.6499563456 | 0.976076555 | 0.74 |\n","| divide\\_by\\_100 | 19 | 3000 | 0.0002 | 0.9 | 0.16 | 0.2360206314 | 0.6092091203 | 0.956937799 | 0.72 |\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.3681054251 | 0.5628615022 | 0.9138755981 | 0.76 | Shuffle=True | Normalize=False |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.65 | 0.13 | 0.3789019991 | 0.6263735294 | 0.966507177 | 0.62 | Shuffle=True | Normalize=False |\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_100 | 11 | 3000 | 0.0001 | 0.8 | 0.15 | 0.2862188623 | 0.6015170813 | 0.956937799 | 0.74 |\n","| divide\\_by\\_100 | 19 | 3000 | 0.00017 | 0.8 | 0.15 | 0.2820716 | 0.6055393815 | 0.961722488 | 0.72 |\n","\n","\n","4. Activation Functions.\n","* Most of the iterations were done on Relu and some were tried on Leaky relu with parameter = 0.01. Softplus(logrithmic version near origin and x < 0 was tried.). Below are some results.  \n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 4000 | 0.003 | 0.65 | 0.13 | 0.3534180278 | 0.4922493398 | 0.9330143541 | 0.8 | Shuffle=False | Normalize=False |\n","| sgdm | lrelu | divide\\_by\\_prod | 19 | 4000 | 0.003 | 0.65 | 0.13 | 0.1613383862 | 0.6416496038 | 0.961722488 | 0.72 | Shuffle=False | Normalize=False |\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | lrelu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.2 | 0.2721504989 | 0.6085265875 | 0.976076555 | 0.7 | Shuffle=True | Normalize=False | Augment=False |\n","| sgdm | softplus | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.2 | 0.2880446532 | 0.6287472844 | 0.971291866 | 0.74 | Shuffle=True | Normalize=False | Augment=False |\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | lrelu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.1609131138 | 0.6363658309 | 0.956937799 | 0.72 | Shuffle=False | Normalize=False |\n","| sgdm | lrelu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.2 | 0.2550611848 | 0.6633317471 | 0.9186602871 | 0.74 | Shuffle=False | Normalize=False |\n","\n","\n","5. Optimizers.\n","* Due to lack of time, ADAM couldn't be setup up correctly to run well. Here is the result from the test conducted.\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | softplus | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.23 | 0.2891651785 | 0.6658042073 | 0.9138755981 | 0.66 | Shuffle=False | Normalize=False | Augment=False |\n","| adam | relu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.23 | 0.6493811878 | 0.8137515187 | 0.6555023923 | 0.34 | Shuffle=False | Normalize=False | Augment=False |\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| adam | relu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.3 | 0.23 | 0.6553247138 | 1.021014094 | 0.6555023923 | 0.34 | Shuffle=False | Normalize=False | Augment=False |\n","| adam | relu | divide\\_by\\_prod | 19 | 2000 | 0.0005 | 0.3 | 0 | 0.646532541 | 0.8413001895 | 0.6555023923 | 0.34 | Shuffle=False | Normalize=False | Augment=False |\n","\n","\n","6. Data Manipulation.\n","* As mentioned above, Image flip, Normalization and random shuffling were tried. The trends were far from expected. Here are the results below. \n","Change in initializations :\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.3561758264 | 0.4837650061 | 0.8947368421 | 0.82 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.2541868931 | 0.7055911422 | 0.971291866 | 0.72 | Shuffle=False | Normalize=True | Augment=False |\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.65 | 0.13 | 0.3789019991 | 0.6263735294 | 0.966507177 | 0.62 | Shuffle=True | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 2000 | 0.003 | 0.65 | 0.13 | 0.363500671 | 0.530864656 | 0.956937799 | 0.78 | Shuffle=False | Normalize=False | Augment=False |\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augement Data** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.3561758264 | 0.4837650061 | 0.8947368421 | 0.82 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 3000 | 0.003 | 0.65 | 0.13 | 0.4042834545 | 0.648612082 | 0.9401913876 | 0.64 | Shuffle=False | Normalize=False | Augment=True |\n","\n","###  **MOVIE REVIEW CLASSIFIER**\n","\n","* Just like the above results, the results for movie classifier is trained. Here are some of the trends. \n","\n","Best result\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 800 | 2000 | 0.01 | 0 | 0 | 0.2558433414 | 0.3977315724 | 0.95375 | 0.855721393 |\n","\n","\n","\n","\n","\n","| **Weight\\_init** | **Batch\\_size** | **Epochs** | **Learning\\_Rate** | **Momentum** | **Weight\\_decay(L2)** | **Train\\_Loss** | **Test\\_Loss** | **Train\\_accuracy** | **Test\\_accuracy** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| divide\\_by\\_prod | 209 | 2000 | 0.0075 | 0.65 | 0.15 | 0.1943106055 | 0.6770002842 | 0.980861244 | 0.72 |\n","| divide\\_by\\_prod | 800 | 1 | 0.0075 | 0 | 0 | 0.7290261388 | 0.7076601982 | 0.5 | 0.5024875622 |\n","| divide\\_by\\_prod | 800 | 2000 | 0.01 | 0 | 0 | 0.2558433414 | 0.3977315724 | 0.95375 | 0.855721393 |\n","\n","\n","\n","\n","\n","| **Optimizer** | **Activation1** | **weight\\_init** | **batch\\_size** | **epochs** | **learning\\_rate** | **momentum** | **weight\\_decay** | **train\\_loss** | **test\\_loss** | **train\\_acc** | **test\\_acc** | **Shuffle** | **Normalize** | **Augment** |\n","| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n","| sgdm | relu | divide\\_by\\_prod | 19 | 4000 | 0.0005 | 0 | 0.19 | 0.5806348774 | 0.6241386533 | 0.9125 | 0.7960199005 | Shuffle=True | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2000 | 0.01 | 0 | 0 | 0.1482970268 | 0.4090732634 | 0.97875 | 0.8109452736 | Shuffle=True | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2000 | 0.01 | 0 | 0 | 0.1482970268 | 0.4090732634 | 0.97875 | 0.8109452736 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2400 | 0.01 | 0 | 0 | 0.1095552891 | 0.4087891579 | 0.99375 | 0.8109452736 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2400 | 0.01 | 0 | 0.1 | 0.394879669 | 0.507001698 | 0.94375 | 0.8009950249 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2400 | 0.01 | 0 | 0.03 | 0.194884792 | 0.4159950912 | 0.97625 | 0.8109452736 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2400 | 0.008 | 0 | 0.03 | 0.2381031513 | 0.4295300245 | 0.96125 | 0.7960199005 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 800 | 2400 | 0.01 | 0.2 | 0.03 | 0.1641093493 | 0.4095226824 | 0.9925 | 0.8208955224 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | relu | divide\\_by\\_prod | 200 | 2400 | 0.0025 | 0.2 | 0.03 | 0.163932655 | 0.4093410373 | 0.9925 | 0.8208955224 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | lrelu | divide\\_by\\_prod | 200 | 2400 | 0.0025 | 0.2 | 0.03 | 0.163925387 | 0.4093404412 | 0.9925 | 0.8208955224 | Shuffle=False | Normalize=False | Augment=False |\n","| sgdm | softplus | divide\\_by\\_prod | 200 | 2400 | 0.0025 | 0.2 | 0.03 | 0.2818528786 | 0.4473121166 | 0.94875 | 0.8009950249 | Shuffle=False | Normalize=False | Augment=False |\n","| adam | relu | divide\\_by\\_prod | 200 | 500 | 0.0025 | 0.2 | 0.03 | 0.1230824292 | 0.4133555591 | 1 | 0.8308457711 | Shuffle=False | Normalize=False | Augment=False |\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Vz2fQLb3wV8","colab_type":"text"},"source":["## Import essential libraries"]},{"cell_type":"code","metadata":{"id":"bjrx9CTUurrS","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import h5py\n","import time\n","import matplotlib.pyplot as plt\n","from googleapiclient.http import *\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import re\n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n","\n","\n","global model, optimizer, loss_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjnjYQtOMcqY","colab_type":"text"},"source":["### **Importing the Data set**\n"]},{"cell_type":"code","metadata":{"id":"Wo94QCiVMnpK","colab_type":"code","outputId":"76311424-de4b-47cc-9905-c324b3746d0c","executionInfo":{"status":"ok","timestamp":1586389331003,"user_tz":240,"elapsed":1482,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pwd\n","%cd drive/My\\ Drive\n","\n","!cp Colab\\ Notebooks/hw3_vishnuu.ipynb ./\n","!jupyter nbconvert --to PDF \"hw3_vishnuu.ipynb\"\n","\n","\n","import os\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jwFVZMruMysj","colab_type":"code","colab":{}},"source":["def load_img_data(train_file, test_file, augment = False):\n","    # Load the training data\n","    train_dataset = h5py.File(train_file, 'r')\n","    \n","    # Separate features(x) and labels(y) for training set\n","    train_set_x_orig = train_dataset['train_set_x']\n","    train_set_y_orig = train_dataset['train_set_y']\n","\n","    # Load the test data\n","    test_dataset = h5py.File(test_file)\n","    \n","    # Separate features(x) and labels(y) for training set\n","    test_set_x_orig = test_dataset['test_set_x']\n","    test_set_y_orig = test_dataset['test_set_y']\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = np.array(train_set_y_orig[:])\n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = np.array(test_set_y_orig[:])\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","\n","    if augment == True:\n","      transform = transforms.RandomHorizontalFlip(p=1)\n","      imgs = np.zeros(train_set_x_orig.shape, dtype = np.uint8)\n","      for i in range(train_set_x_orig.shape[0]):\n","        imgs[i,:] = np.array(transform(Image.fromarray(train_set_x_orig[i,:])), dtype=np.uint8)\n","      train_set_x_orig = np.append(train_set_x_orig, imgs, axis = 0)\n","      train_set_y_orig = np.append(train_set_y_orig, train_set_y_orig, axis = 1)\n","\n","               \n","\n","    train_x_flatten = np.array(train_set_x_orig).reshape(train_set_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n","    test_x_flatten = np.array(test_set_x_orig).reshape(test_set_x_orig.shape[0], -1).T\n","\n","    train_data = torch.tensor(train_x_flatten/255.).float()\n","    test_data = torch.tensor(test_x_flatten/255.).float()\n","    # train_data = train_x_flatten/255.\n","    # test_data = test_x_flatten/255.\n","    train_label = torch.tensor(train_set_y_orig).float()\n","    test_label = torch.tensor(test_set_y_orig).float()\n","    \n","    return train_data, train_label, test_data, test_label\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jolPKYTMzuj","colab_type":"code","colab":{}},"source":["train_file=\"data/train_catvnoncat.h5\"\n","test_file=\"data/test_catvnoncat.h5\"\n","# print(os.getcwd())\n","train_data, train_label, test_data, test_label = load_img_data(train_file, test_file, augment = True)\n","print(train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKgkmHrD6Xe2","colab_type":"text"},"source":["**Normalize image data**\n"]},{"cell_type":"code","metadata":{"id":"OxeOKol95aGX","colab_type":"code","colab":{}},"source":["def normalize_img_data(data):\n","  data_mean = torch.mean(data, axis = 1)\n","  data = data - data_mean[:, None]\n","  # print(type(data))\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfBhvT08gHvg","colab_type":"text"},"source":["**Random Shuffle of data**"]},{"cell_type":"code","metadata":{"id":"uGX725mYgKoG","colab_type":"code","colab":{}},"source":["def random_shuffle(data, label):\n","  rand_idx = torch.randperm(data.shape[1])\n","  data = data[:,rand_idx]\n","  label = label[:, rand_idx]\n","  # print(data[:,0])\n","  return data, label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOVGNtjoEP-K","colab_type":"text"},"source":["### **We define a nueral net model by inheriting nn.Module from Torch's libraries.**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cq4O2DMZMcNy","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"WVJR1i7BEqei","colab_type":"code","colab":{}},"source":["class net(nn.Module):\n","  \n","  def __init__(self, n1, n2, nx, act1):\n","    torch.manual_seed(0)\n","    super(net, self).__init__()\n","    self.fc1 = nn.Linear(nx, n1).float()\n","    self.fc1.weights = nn.Parameter(torch.randn(nx,n1)/(nx*n1))\n","    self.fc1.bias = nn.Parameter(torch.randn(n1))\n","    self.fc2 = nn.Linear(n1,n2).float()\n","    self.fc2.weights = nn.Parameter(torch.randn(n1,n2)/(n1*n2))\n","    self.fc2.bias = nn.Parameter(torch.randn(n2))\n","    self.act1 = act1\n","    \n","  def forward(self,X):\n","    if self.act1 == \"relu\":\n","      A1 = F.relu(self.fc1(X))\n","    elif self.act1 == \"lrelu\":\n","      A1 = F.leaky_relu(self.fc1(X), negative_slope=0.01)\n","    elif self.act1 == \"softplus\":\n","      A1 = F.softplus(self.fc1(X), beta = 1, threshold = 5)\n","    A2 = torch.sigmoid(self.fc2(A1))\n","\n","    return A2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTtN9Js0TLaU","colab_type":"text"},"source":["### **We define the hyper parameters next**. "]},{"cell_type":"code","metadata":{"id":"pi4EwCnYTKN4","colab_type":"code","colab":{}},"source":["nx = train_data.shape[0] # feature size of the input\n","n1 = 7     # number of nuerons in first layer\n","n2 = 1     # number of nuerons in the final layer\n","learning_rate = 0.0075 # Setting the Learning rate\n","momentum = 0.8         # Momentum for SGD with momentum\n","bs = 209     # Entire Dataset\n","ep = 2500        # Number of times entire training data set is seen\n","weight_decay = 0.05 # L2 normalization parameter\n","weight_init = \"xavier\"\n","optimizer_name = \"sgdm\"\n","# print (nx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgM5Ej_8W0J9","colab_type":"text"},"source":["### **Now we design our nueral net and instantiate it.**"]},{"cell_type":"code","metadata":{"id":"XtLF_APnXsBO","colab_type":"code","colab":{}},"source":["  def net_init(nx, n1, n2, act1, optimizer_name):\n","    global model, optimizer, loss_fn\n","    if optimizer_name == \"sgdm\":\n","      model = net(n1, n2, nx, act1)\n","      optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay=weight_decay)\n","      loss_fn = nn.BCELoss()\n","    if optimizer_name == \"adam\":\n","      model = net(n1, n2, nx, act1)\n","      optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas = betas_adam, weight_decay= weight_decay)\n","      loss_fn = nn.BCELoss()  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcFd4TdBi5--","colab_type":"text"},"source":["### **Training the Model**"]},{"cell_type":"code","metadata":{"id":"BfpbUW2SjvIz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":137},"outputId":"346eb13e-5b53-4d58-e1c3-5ac578099d4b","executionInfo":{"status":"error","timestamp":1586403353360,"user_tz":240,"elapsed":699,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}}},"source":["def train_model(train_data, train_label, batch_size = 209, epochs = 2500, plot = True, normalize = False, shuffle= False, augment = False):\n","  global model, optimizer, loss_fn\n","  if augment_data = True:\n","    train_data = augment_data(train_data)\n","  if normalize == True:\n","    train_data = normalize_img_data(train_data)\n","\n","  t0 = time.time()\n","  train_loss = []\n","  for epoch in range(epochs):\n","    model.train()   # setting model to training phase\n","    train_epoch_loss = []\n","    if shuffle==True:\n","      train_data, train_label = random_shuffle(train_data, train_label)\n","\n","    for i in range(0, train_data.shape[1], batch_size):\n","      tdata = train_data[:,i :  i+batch_size-1].T\n","      ldata = train_label[:,i : i+batch_size-1].T\n","      # print(tdata.shape)\n","      model_output = model(tdata)\n","      loss = loss_fn(model_output, ldata)\n","      # print(loss.item())\n","      train_epoch_loss.append(loss.item())\n","      # print(train_loss)\n","      optimizer.zero_grad()\n","      # print(type(train_loss))\n","      loss.backward()\n","      optimizer.step()\n","\n","    train_loss.append(float(sum(train_epoch_loss))/float(len(train_epoch_loss)))  \n","\n","    if epoch%100 == 0:\n","      # print(train_loss)\n","      print(\"Epoch: \" + str(epoch) + \", Training Loss: \" + str(float(sum(train_epoch_loss))/float(len(train_epoch_loss))))\n","  tloss = float(sum(train_epoch_loss))/float(len(train_epoch_loss))\n","  print(\"Epoch: \" + str(epoch) + \", Training Loss: \" + str(tloss))       \n","  # print(len(train_loss))\n","  fig = plt.figure()\n","  if plot:\n","    plt.plot(np.squeeze(train_loss), 'b')\n","    plt.ylabel('loss')\n","    plt.xlabel('Number of batches')\n","    plt.title('Loss plot, Learning rate: {}, Epochs: {} '.format(learning_rate, epochs)  )\n","    plt.show() \n","  \n","  return tloss, batch_size, epochs, fig"],"execution_count":377,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-378-464f96e193e1>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    if augment_data = True:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"DTelsOkf-hA-","colab_type":"code","colab":{}},"source":["net_init(nx, n1, n2, \"relu\", \"sgdm\")\n","train_loss, batch_size, epochs, fig = train_model(train_data, train_label, 209, 2000)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOvAFQnO9MFA","colab_type":"text"},"source":["### **Running on Test Set**"]},{"cell_type":"code","metadata":{"id":"nbXu8IQc9Y7Y","colab_type":"code","colab":{}},"source":["def test_model(tdata, ldata, normalize = False):\n","  if normalize == True:\n","    tdata = normalize_img_data(tdata)\n","  t0 = time.time()\n","  model.eval()\n","  test_loss = []\n","  with torch.no_grad():\n","    # print(tdata.shape)\n","    model_output = model(tdata.T)\n","    loss = loss_fn(model_output, ldata.T)\n","    \n","    \n","    test_loss.append(loss.item())\n","  test_loss = sum(test_loss)/len(test_loss)\n","  \n","  return test_loss   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82hcgnNR5SR1","colab_type":"code","colab":{}},"source":["test_loss = test_model(test_data,test_label)\n","print(\"Testing Loss: \" + str(test_loss))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_MRYP43DIcg","colab_type":"text"},"source":["### **Find the Training and Testing Accuracies**"]},{"cell_type":"code","metadata":{"id":"7_Iv-BYlDXkU","colab_type":"code","colab":{}},"source":["def train_accuracy(tdata, ldata, normalize = False):\n","  if normalize == True:\n","    tdata = normalize_img_data(tdata)\n","  model.eval()\n","\n","  with torch.no_grad():\n","    model_output = model(tdata.T)\n","    probas = np.zeros(model_output.shape)\n","    probas = np.where(model_output > 0.5, 1, 0)\n","    # print(ldata)\n","    trA = np.mean(np.where( (probas - ldata.T.data.numpy())  == 0, 1, 0))\n","    \n","    return trA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvC6QtKAImtf","colab_type":"code","outputId":"a82c545e-35bb-48e4-888d-8b12c0034dcb","executionInfo":{"status":"ok","timestamp":1586327408414,"user_tz":240,"elapsed":428,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["trA = train_accuracy(train_data, train_label)\n","print(\"Training Accuracy seen: \" + str(trA))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Accuracy seen: 0.9952153110047847\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EB5p5Xk2GgYC","colab_type":"code","colab":{}},"source":["def test_accuracy(tdata, ldata, normalize = False):\n","  if normalize == True:\n","    tdata = normalize_img_data(tdata)\n","  model.eval()\n","  with torch.no_grad():\n","    model_output = model(tdata.T)\n","    probas = np.zeros(model_output.shape)\n","    probas = np.where(model_output > 0.5, 1, 0)\n","    # print(ldata)\n","    teA = np.mean(np.where( (probas - ldata.T.data.numpy())  == 0, 1, 0))\n","    \n","    return teA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zCpevOaLxqa","colab_type":"code","outputId":"0d502a10-8783-45b4-a500-428848c5724c","executionInfo":{"status":"ok","timestamp":1586327411553,"user_tz":240,"elapsed":494,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["teA = test_accuracy(test_data, test_label)\n","print(\"Testing Accuracy seen: \" + str(teA))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy seen: 0.7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"47J_UFNqBi-f","colab_type":"text"},"source":["## **Model Training & Testing**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y_4V4pgpWVQ4","colab":{}},"source":["nx = train_data.shape[0] # feature size of the input\n","n1 = 7     # number of nuerons in first layer\n","n2 = 1     # number of nuerons in the final layer\n","learning_rate = 0.0005 # Setting the Learning rate\n","momentum = 0.3   # Momentum for SGD with momentum\n","bs = 19              # Batch size\n","ep = 2000       # Number of times entire training data set is seen\n","weight_decay = 0 # L2 normalization parameter\n","weight_init = \"divide_by_prod\"\n","optimizer_name = \"adam\"\n","betas_adam = (0.9, 0.999)\n","activation1 = \"relu\" # relu, lrelu, prelu, softplus\n","normalize = False\n","shuffle = False\n","augment = False\n","train_file=\"data/train_catvnoncat.h5\"\n","test_file=\"data/test_catvnoncat.h5\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtOzJkulofBo","colab_type":"code","colab":{}},"source":["train_data, train_label, test_data, test_label = load_img_data(train_file, test_file, augment=augment)\n","net_init(nx, n1, n2, activation1, optimizer_name)\n","train_loss, batch_size, epochs, fig = train_model(train_data, train_label, bs, ep, shuffle=shuffle, normalize = normalize)\n","test_loss = test_model(test_data,test_label, normalize = normalize)\n","train_acc = train_accuracy(train_data, train_label, normalize = normalize)\n","test_acc = test_accuracy(test_data, test_label, normalize = normalize)\n","# Weight_init, Batch_size, Epochs, Learning_Rate, Momentum, Weight_decay(L2), Train_Loss, Test_Loss, Train_accuracy, Test_accuracy\n","\n","\n","res_file = open(\"hw3_deeplearning/results.txt\",\"a\")\n","res_file.write(optimizer_name +  \",\" + activation1 + \",\" + str(weight_init) + \",\" + str(batch_size) +\",\"+ str(epochs) + \",\" + str(learning_rate) + \",\" + str(momentum)+\",\" + \n","              str(weight_decay) +\",\" + str(train_loss) + \",\" + str(test_loss) + \",\" + str(train_acc) +\",\"+ str(test_acc) +  \",\"+ \"Shuffle=\" + str(shuffle) + \",\" + \"Normalize=\" + str(normalize) + \",\"+\"Augment=\" + str(augment) + \"\\n\"  )\n","res_file.close()\n","\n","\n","print(\"Optimizer: \" + optimizer_name + \"\\n\" +\n","      \"Betas Adam\" + str(betas_adam) + \"\\n\" +\n","      \"Weight_init:  \" + weight_init +\"\\n\"+\n","      \"Batch_size: \" + str(batch_size)  +\"\\n\" + \n","      \"Epochs: \" + str(epochs) +\"\\n\"+\n","      \"Learning_Rate: \" + str(learning_rate) +\"\\n\"+\n","      \"Momentum: \" + str(momentum) +\"\\n\"+ \n","      \"Weight_decay(L2): \" + str(weight_decay) +\"\\n\" +\n","      \"Train_Loss: \" + str(train_loss) +\"\\n\"+\n","      \"Test_Loss: \" + str(test_loss) +\"\\n\" + \n","      \"Train_accuracy: \" + str(train_acc) +\"\\n\" + \n","      \"Test_accuracy: \" + str(test_acc) + \"\\n\" +\n","      \"Normalize: \" +  str(normalize) + \"\\n\" +\n","      \"Shuffle Data:\" + str(shuffle) + \"\\n\" + \n","      \"Augment Data:\" + str(augment) + \"\\n\"\n","      )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olJ_w9Ne5Nbi","colab_type":"text"},"source":["----\n","#MOVIE REVIEW"]},{"cell_type":"code","metadata":{"id":"KRIfJ5KvCbok","colab_type":"code","colab":{}},"source":["def load_data(train_file, test_file):\n","    train_dataset = []\n","    test_dataset = []\n","    \n","    # Read the training dataset file line by line\n","    for line in open(train_file, 'r'):\n","        train_dataset.append(line.strip())\n","        \n","    for line in open(test_file, 'r'):\n","        test_dataset.append(line.strip())\n","    return train_dataset, test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NN8BXR9bSrNl","colab_type":"code","colab":{}},"source":["train_file = \"data/train_imdb.txt\"\n","test_file = \"data/test_imdb.txt\"\n","train_dataset, test_dataset = load_data(train_file, test_file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAQL8hAiCj7E","colab_type":"code","colab":{}},"source":["# This is just how the data is organized. The first 50% data is positive and the rest 50% is negative for both train and test splits.\n","y = [1 if i < len(train_dataset)*0.5 else 0 for i in range(len(train_dataset))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YxXIg-cCmQ1","colab_type":"code","outputId":"8755c7a1-8f03-4109-e126-0258228a7c31","executionInfo":{"status":"ok","timestamp":1586401460228,"user_tz":240,"elapsed":505,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["# Example of a review\n","index = 0\n","print(train_dataset[index])\n","print (\"y = \" + str(y[index]))"],"execution_count":360,"outputs":[{"output_type":"stream","text":["Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n","y = 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NKA5oQLZSrNu","colab_type":"code","outputId":"21d9c06a-68d0-4e79-d1a6-1ce9dddf1499","executionInfo":{"status":"ok","timestamp":1586401462281,"user_tz":240,"elapsed":620,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Explore your dataset \n","m_train = len(train_dataset)\n","m_test = len(test_dataset)\n","\n","print (\"Number of training examples: \" + str(m_train))\n","print (\"Number of testing examples: \" + str(m_test))"],"execution_count":361,"outputs":[{"output_type":"stream","text":["Number of training examples: 1001\n","Number of testing examples: 201\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"asZP-jHnSrNv","colab_type":"text"},"source":["## Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"cn_Zyq4DSrNx","colab_type":"text"},"source":["From the example review, you can see that the raw data is really noisy! This is generally the case with the text data. Hence, Preprocessing the raw input and cleaning the text is  essential. Please run the code snippet provided below.\n","\n","**Exercise**: Explain what pattern the model is trying to capture using re.compile in your report. "]},{"cell_type":"code","metadata":{"id":"kOgW9oFzSrNx","colab_type":"code","colab":{}},"source":["REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n","REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n","NO_SPACE = \"\"\n","SPACE = \" \"\n","\n","def preprocess_reviews(reviews):\n","    \n","    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n","    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n","    \n","    return reviews\n","\n","train_dataset_clean = preprocess_reviews(train_dataset)\n","test_dataset_clean = preprocess_reviews(test_dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxkrG7J5SrN0","colab_type":"code","outputId":"252c766f-c9d3-4be9-bb62-22a16c6848c8","executionInfo":{"status":"ok","timestamp":1586401466200,"user_tz":240,"elapsed":177,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["# Example of a clean review\n","index = 7\n","print(train_dataset_clean[index])\n","print (\"y = \" + str(y[index]))"],"execution_count":363,"outputs":[{"output_type":"stream","text":["in this critically acclaimed psychological thriller based on true events gabriel robin williams a celebrated writer and late night talk show host becomes captivated by the harrowing story of a young listener and his adoptive mother toni collette when troubling questions arise about this boys story however gabriel finds himself drawn into a widening mystery that hides a deadly secretÂ… according to films official synopsis you really should stop reading these comments and watch the film now the how did he lose his leg ending with ms collette planning her new life should be chopped off and sent to deleted scenes land its overkill the true nature of her physical and mental ailments should be obvious by the time mr williams returns to new york possibly her blindness could be in question   but a revelation could have be made certain in either the highway or video tape scenes the film would benefit from a re editing   how about a directors cut  williams and bobby cannavale as jess dont seem initially believable as a couple a scene or two establishing their relationship might have helped set the stage otherwise the cast is exemplary williams offers an exceptionally strong characterization and not a gay impersonation sandra oh as anna joe morton as ashe and rory culkin pete logand are all perfect best of all collettes donna belongs in the creepy hall of fame ms oh is correct in saying collette might be you know like that guy from psycho there have been several years when organizations giving acting awards seemed to reach for women due to a slighter dispersion of roles certainly they could have noticed collette with some award consideration she is that good and director patrick stettner definitely evokes hitchcock   he even makes getting a sandwich from a vending machine suspenseful finally writers stettner armistead maupin and terry anderson deserve gratitude from flight attendants everywhere ******* the night listener    patrick stettner ~ robin williams toni collette sandra oh rory culkin\n","y = 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FFFkHd5SSrN3","colab_type":"text"},"source":["## Vectorization"]},{"cell_type":"markdown","metadata":{"id":"AjBWOK6GSrN3","colab_type":"text"},"source":["# Now lets create a feature vector for our reviews based on a simple bag of words model. So, given an input text, we need to create a numerical vector which is simply the vector of word counts for each word of the vocabulary. Run the code below to get the feature representation."]},{"cell_type":"code","metadata":{"id":"pP0giep1SrN4","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","cv = CountVectorizer(binary=True, stop_words=\"english\", max_features=2000)\n","cv.fit(train_dataset_clean)\n","X = cv.transform(train_dataset_clean)\n","X_test = cv.transform(test_dataset_clean)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkBSZuJiSrN8","colab_type":"text"},"source":["CountVectorizer provides a sparse feature representation by default which is reasonable because only some words occur in individual example. However, for training neural network models, we generally use a dense representation vector."]},{"cell_type":"code","metadata":{"id":"wUsqaytjSrN8","colab_type":"code","colab":{}},"source":["X = np.array(X.todense()).astype(float)\n","X_test = np.array(X_test.todense()).astype(float)\n","y = np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qJPgeyLSrN_","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"yJw7RTD6SrOA","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y, train_size = 0.80\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LotwEdJISrOF","colab_type":"code","colab":{}},"source":["# This is just to correct the shape of the arrays as required by the two_layer_model\n","X_train = torch.tensor(X_train.T).float()\n","X_val = torch.tensor(X_val.T).float()\n","y_train = torch.tensor(y_train.reshape(1,-1)).float()\n","y_val = torch.tensor(y_val.reshape(1,-1)).float()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRCNeUg8SrOI","colab_type":"code","colab":{}},"source":["### CONSTANTS DEFINING THE MODEL ####\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tU-Y8_S-DRG3","colab_type":"code","colab":{}},"source":["n_x = X_train.shape[0]    \n","n_1 = 200     # number of nuerons in first layer\n","n_2 = 1     # number of nuerons in the final layer\n","learning_rate = 0.0025 # Setting the Learning rate\n","momentum = 0.2  # Momentum for SGD with momentum\n","bs = 200             # Batch size\n","ep = 500       # Number of times entire training data set is seen\n","weight_decay = 0.03 #L2 normalization parameter\n","weight_init = \"divide_by_prod\"\n","optimizer_name = \"adam\"\n","betas_adam = (0.9, 0.999)\n","activation1 = \"relu\" # relu, lrelu, prelu, softplus\n","normalize = False \n","shuffle = False \n","augment = False # cannot be augmented here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDN3GBBZDVrI","colab_type":"code","outputId":"f3c4458b-6654-40bf-805d-478821fe02f4","executionInfo":{"status":"ok","timestamp":1586404603042,"user_tz":240,"elapsed":18018,"user":{"displayName":"Vishnuu Appaya Dhanabalan","photoUrl":"","userId":"12368386922281579546"}},"colab":{"base_uri":"https://localhost:8080/","height":706}},"source":["net_init(n_x, n_1, n_2, activation1, optimizer_name)\n","train_loss, batch_size, epochs, fig = train_model(X_train, y_train, bs, ep, shuffle=shuffle,  normalize = normalize)\n","test_loss = test_model(X_val,y_val)\n","train_acc = train_accuracy(X_train, y_train)\n","test_acc = test_accuracy(X_val, y_val)\n","# Weight_init, Batch_size, Epochs, Learning_Rate, Momentum, Weight_decay(L2), Train_Loss, Test_Loss, Train_accuracy, Test_accuracy\n","\n","res_file = open(\"hw3_deeplearning/results_movie_reviews.txt\",\"a\")\n","res_file.write(optimizer_name +  \",\" + activation1 + \",\" + str(weight_init) + \",\" + str(batch_size) +\",\"+ str(epochs) + \",\" + str(learning_rate) + \",\" + str(momentum)+\",\" + \n","              str(weight_decay) +\",\" + str(train_loss) + \",\" + str(test_loss) + \",\" + str(train_acc) +\",\"+ str(test_acc) +  \",\"+ \"Shuffle=\" + str(shuffle) + \",\" + \"Normalize=\" + str(normalize) + \",\"+\"Augment=\" + str(augment) + \"\\n\"  )\n","res_file.close()\n","\n","\n","print(\"Optimizer: \" + optimizer_name + \"\\n\" +\n","      \"Betas Adam\" + str(betas_adam) + \"\\n\" +\n","      \"Weight_init:  \" + weight_init +\"\\n\"+\n","      \"Batch_size: \" + str(batch_size)  +\"\\n\" + \n","      \"Epochs: \" + str(epochs) +\"\\n\"+\n","      \"Learning_Rate: \" + str(learning_rate) +\"\\n\"+\n","      \"Momentum: \" + str(momentum) +\"\\n\"+ \n","      \"Weight_decay(L2): \" + str(weight_decay) +\"\\n\" +\n","      \"Train_Loss: \" + str(train_loss) +\"\\n\"+\n","      \"Test_Loss: \" + str(test_loss) +\"\\n\" + \n","      \"Train_accuracy: \" + str(train_acc) +\"\\n\" + \n","      \"Test_accuracy: \" + str(test_acc) + \"\\n\" +\n","      \"Normalize: \" +  str(normalize) + \"\\n\" +\n","      \"Shuffle Data:\" + str(shuffle) + \"\\n\" + \n","      \"Augment Data:\" + str(augment) + \"\\n\"\n","      )"],"execution_count":398,"outputs":[{"output_type":"stream","text":["Epoch: 0, Training Loss: 0.6810003072023392\n","Epoch: 100, Training Loss: 0.12238629907369614\n","Epoch: 200, Training Loss: 0.12292882055044174\n","Epoch: 300, Training Loss: 0.123882956802845\n","Epoch: 400, Training Loss: 0.121922817081213\n","Epoch: 499, Training Loss: 0.12308242917060852\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUoAAAEWCAYAAAAAZd6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVkZ/wfB1P5jGDQuPgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 360x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Optimizer: adam\n","Betas Adam(0.9, 0.999)\n","Weight_init:  divide_by_prod\n","Batch_size: 200\n","Epochs: 500\n","Learning_Rate: 0.0025\n","Momentum: 0.2\n","Weight_decay(L2): 0.03\n","Train_Loss: 0.12308242917060852\n","Test_Loss: 0.4133555591106415\n","Train_accuracy: 1.0\n","Test_accuracy: 0.8308457711442786\n","Normalize: False\n","Shuffle Data:False\n","Augment Data:False\n","\n"],"name":"stdout"}]}]}